import torch
import torch.nn as nn
import torch.nn.functional as F
from models.pvtv2 import pvt_v2_b2


class ConvBR(nn.Module):
    def __init__(self, in_channel, out_channel, kernel_size, stride=1, padding=0, dilation=1):
        super(ConvBR, self).__init__()
        self.conv = nn.Conv2d(in_channel, out_channel,
                              kernel_size=kernel_size, stride=stride,
                              padding=padding, dilation=dilation, bias=False)
        self.bn = nn.BatchNorm2d(out_channel)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        return x


class DimensionalReduction(nn.Module):
    def __init__(self, in_channel, out_channel):
        super(DimensionalReduction, self).__init__()
        self.reduce = nn.Sequential(
            ConvBR(in_channel, out_channel, 3, padding=1),
            ConvBR(out_channel, out_channel, 3, padding=1)
        )

    def forward(self, x):
        return self.reduce(x)
class SimpleDecoder(nn.Module):
    def __init__(self, channel):
        super(SimpleDecoder, self).__init__()

        # é€æ­¥ä¸Šé‡‡æ ?
        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)

        # èåˆç‰¹å¾
        self.fusion_conv = ConvBR(channel * 4, channel, 3, padding=1)
        self.fusion_conv1 = ConvBR(channel * 2, channel, 3, padding=1)
        self.fusion_conv2 = ConvBR(channel * 2, channel, 3, padding=1)
        self.fusion_conv3 = ConvBR(channel * 2, channel, 3, padding=1)

        self.output_conv1 = nn.Conv2d(channel, 2*channel, kernel_size=3)  # è¾“å‡º5ä¸ªç±»åˆ?
        self.output_conv2 = nn.Conv2d(2*channel, channel, kernel_size=3)  # è¾“å‡º5ä¸ªç±»åˆ?
        self.output_conv = nn.Conv2d(channel, channel, kernel_size=3)  # è¾“å‡º5ä¸ªç±»åˆ?

        self.output_conv_fixation = nn.Sequential(
            # ç¬¬ä¸€å±‚å·ç§?(ä¿æŒå°ºå¯¸)
            nn.Conv2d(channel, channel, 3, padding=1, bias=False),
            nn.BatchNorm2d(channel),
            nn.ReLU(inplace=True),

            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),

            # ç¬¬äºŒå±‚å·ç§?(ä¿æŒå°ºå¯¸)
            nn.Conv2d(channel, channel//2, 3, padding=1, bias=False),
            nn.BatchNorm2d(channel//2),
            nn.ReLU(inplace=True),

            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),

            # æœ€ç»ˆè¾“å‡ºå±‚
            nn.Conv2d(channel//2, 1, 1),  # 1x1å·ç§¯å‹ç¼©åˆ°å•é€šé“
            nn.Sigmoid()  # è¾“å‡º0-1
        )

        # å…¨å±€å¹³å‡æ± åŒ–å±?
        self.global_pool = nn.AdaptiveAvgPool2d(1)  # è¾“å‡º[batch, 5, 1, 1]

        # ä¿®æ”¹å…¨è¿æ¥å±‚çš„è¾“å…¥ç»´åº¦ä¸º channel * 5
        self.classify = nn.Conv2d(channel, 6, kernel_size=1)  # è¾“å‡º5ä¸ªç±»åˆ?

        # Softmaxå±‚ï¼Œç”¨äºè®¡ç®—ç±»åˆ«æ¦‚ç‡
        self.softmax = nn.Softmax(dim=1)  # dim=1è¡¨ç¤ºæ²¿ç€ç±»åˆ«ç»´åº¦åº”ç”¨Softmax

    def forward(self, f1, f2, f3, f4, mask):

        # é€æ­¥ä¸Šé‡‡æ ?
        f2 = self.upsample(f2)  # H/8 -> H/4
        f3 = self.upsample(self.upsample(f3))  # H/16 -> H/4
        f4 = self.upsample(self.upsample(self.upsample(f4)))  # H/32 -> H/4

        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾?
        fused1 = torch.cat([f4,f3], dim=1)
        fused1 = self.fusion_conv1(fused1)
        fused2 = torch.cat([fused1,f2], dim=1)
        fused2 = self.fusion_conv2(fused2)
        fused3 = torch.cat([fused2, f1], dim=1)
        fused3 = self.fusion_conv3(fused3)

        # ç”Ÿæˆæœ€ç»ˆè¾“å‡?
        output1 = self.output_conv1(fused3)
        output2 = self.output_conv2(output1)
        logits = self.output_conv(output2)
        logits = F.interpolate(logits, size=(mask.size(2), mask.size(3)), mode='bilinear', align_corners=True)
        logits = self.classify(logits)  # è¾“å‡º [batch, 6,352,352]
        #sem = self.softmax(logits)  # è¾“å‡º [batch, 6,352,352]ï¼Œæ¯ä¸ªå€¼æ˜¯ç±»åˆ«çš„æ¦‚ç?
        fixation = self.output_conv_fixation(fused3)

        # 2. æ‰©å±•maskçš„é€šé“æ•°ï¼Œä»?[batch, 1, 352, 352] åˆ?[batch, 6, 352, 352]ï¼Œä»¥ä¾¿ä¸outputåŒ¹é…
        mask = mask.expand(-1, 6, -1, -1)  # æ‰©å±•é€šé“æ•?
        fixation = fixation.expand(-1, 6, -1, -1)  # æ‰©å±•é€šé“æ•?

        # 3. ç”¨maskå¯¹outputåšé€å…ƒç´ ç‚¹ä¹˜ï¼ˆåŠ æƒï¼?
        masked_logits = logits * mask  # è¾“å‡º [batch, 6, 352, 352]
        masked_logits = masked_logits * fixation
        masked_sum = masked_logits.sum(dim=[2, 3])  # (batch, 6)
        #mask_area = mask.sum(dim=[2, 3]).clamp(min=1e-6)  # (batch, 1)
        fixation_area = (fixation * mask).sum(dim=[2, 3]).clamp(min=1e-6)  # (batch, 1)
        target_logits = masked_sum / fixation_area  # (batch, 6)

        return logits, target_logits

class UEDGNet(nn.Module):
    def __init__(self, channel=64):
        super(UEDGNet, self).__init__()

        # Backbone (PVTv2)
        self.backbone = pvt_v2_b2()

        # åŠ è½½é¢„è®­ç»ƒæƒé‡?
        path = './pre_trained/pvt_v2_b2.pth'
        save_model = torch.load(path)
        model_dict = self.backbone.state_dict()
        state_dict = {k: v for k, v in save_model.items() if k in model_dict.keys()}
        model_dict.update(state_dict)
        self.backbone.load_state_dict(model_dict)

        # ç‰¹å¾ç»´åº¦ç¼©å‡
        self.dr1 = DimensionalReduction(128, channel)  # å¯¹åº”layer2è¾“å‡º
        self.dr2 = DimensionalReduction(320, channel)  # å¯¹åº”layer3è¾“å‡º
        self.dr3 = DimensionalReduction(512, channel)  # å¯¹åº”layer4è¾“å‡º

        self.encoder_fusion = ConvBR(channel * 2, channel, 3, padding=1)

        # ç®€åŒ–è§£ç å™¨
        self.decoder = SimpleDecoder(channel)


    def forward(self, x, mask):
        fore = x * mask
        back = x * (1 - mask)
        # æå–ç‰¹å¾
        pvt_fore = self.backbone(fore)
        pvt_back = self.backbone(back)
        fb1_f = pvt_fore[0]  # [batch, 64, H/4, W/4]
        fb2_f = pvt_fore[1]  # [batch, 128, H/8, W/8]
        fb3_f = pvt_fore[2]  # [batch, 320, H/16, W/16]
        fb4_f = pvt_fore[3]  # [batch, 512, H/32, W/32]

        fb1_b = pvt_back[0]  # [batch, 64, H/4, W/4]
        fb2_b = pvt_back[1]  # [batch, 128, H/8, W/8]
        fb3_b = pvt_back[2]  # [batch, 320, H/16, W/16]
        fb4_b = pvt_back[3]  # [batch, 512, H/32, W/32]

        # ç»´åº¦ç¼©å‡
        xr2_f = self.dr1(fb2_f)
        xr3_f = self.dr2(fb3_f)
        xr4_f = self.dr3(fb4_f)
        xr2_b = self.dr1(fb2_b)
        xr3_b = self.dr2(fb3_b)
        xr4_b = self.dr3(fb4_b)

        fb1 = torch.cat([fb1_f, fb1_b], dim=1)
        xr2 = torch.cat([xr2_f, xr2_b], dim=1)
        xr3 = torch.cat([xr3_f, xr3_b], dim=1)
        xr4 = torch.cat([xr4_f, xr4_b], dim=1)

        fb1 = self.encoder_fusion(fb1)
        xr2 = self.encoder_fusion(xr2)
        xr3 = self.encoder_fusion(xr3)
        xr4 = self.encoder_fusion(xr4)

        # è§£ç å™¨ç”Ÿæˆé¢„æµ?
        pred = self.decoder(fb1, xr2, xr3, xr4, mask)

        return pred


if __name__ == '__main__':
    net = UEDGNet(channel=64).eval()
    inputs = torch.randn(1, 3, 352, 352)
    mask = torch.randn(1, 1, 352, 352)
    sem, output = net(inputs,mask)
    print(sem[:,:,1,1])
    print(output)  # åº”è¯¥è¾“å‡º: torch.Size([1, 5])
