#!/usr/bin/env python3
"""
Comprehensive Report Generator for Bradley-Terry Score Generation Process
========================================================================

This script generates a detailed PDF report explaining the entire score generation
process for the camouflage-saliency scoring system using Bradley-Terry models.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.backends.backend_pdf import PdfPages
from pathlib import Path
import json
import joblib
from scipy.stats import spearmanr, entropy
from matplotlib.patches import Rectangle
import matplotlib.patches as mpatches

# Set style for publication-quality plots
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

def load_data():
    """Load all generated data from the scoring pipeline."""
    base_path = Path("/HDD_16T/rsy/UEDG-master/Ranking_whole/")
    
    # Load main results
    df = pd.read_csv(base_path / "gt_continuous.csv")
    
    # Load metrics
    with open(base_path / "spearman_metrics.json", 'r') as f:
        metrics = json.load(f)
    
    # Load ECDF parameters
    with open(base_path / "ecdf_params.json", 'r') as f:
        ecdf_params = json.load(f)
    
    # Load Bradley-Terry weights
    weights = joblib.load(base_path / "bradley_terry_weights.pkl")
    
    return df, metrics, ecdf_params, weights

def create_title_page(pdf):
    """Create the title page of the report."""
    fig, ax = plt.subplots(figsize=(8.5, 11))
    ax.axis('off')
    
    # Title
    ax.text(0.5, 0.8, 'Bradley-Terry Score Generation\nfor Camouflage-Saliency Ranking', 
            ha='center', va='center', fontsize=24, fontweight='bold')
    
    # Subtitle
    ax.text(0.5, 0.7, 'Comprehensive Technical Report\nScheme B-soft Implementation', 
            ha='center', va='center', fontsize=16, style='italic')
    
    # Authors and affiliation
    ax.text(0.5, 0.55, 'Generated by: Advanced Computer Vision Pipeline\nDate: December 2024', 
            ha='center', va='center', fontsize=12)
    
    # Abstract box
    abstract_text = """
    ABSTRACT
    
    This report presents a comprehensive analysis of the Bradley-Terry model-based scoring system
    for unified camouflage-saliency ranking. The methodology employs a balanced calibration approach
    to generate continuous scores in the range [0, 10], where 0 represents maximum camouflage and
    10 represents maximum saliency. The system processes 38,580 samples (6,575 COD + 32,005 SOD)
    and achieves exceptional correlation with ground truth labels (ρ_COD = 0.979, ρ_SOD = 1.000).
    
    Key innovations include: (1) Balanced comparison generation with cross-domain soft constraints,
    (2) Regularization-free logistic regression for pure maximum-likelihood estimation, and
    (3) Balanced calibration to prevent distribution skew from class imbalance.
    """
    
    ax.text(0.5, 0.3, abstract_text, ha='center', va='center', fontsize=10,
            bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.8))
    
    pdf.savefig(fig, bbox_inches='tight')
    plt.close()

def create_methodology_overview(pdf):
    """Create methodology overview page."""
    fig, ax = plt.subplots(figsize=(8.5, 11))
    ax.axis('off')
    
    # Title
    ax.text(0.5, 0.95, 'METHODOLOGY OVERVIEW', ha='center', va='top', 
            fontsize=18, fontweight='bold')
    
    # Pipeline diagram
    steps = [
        "1. Data Enumeration\n38,580 samples\n(COD: 6,575, SOD: 32,005)",
        "2. Comparison Generation\n5.79M pairwise comparisons\n• SOD within-image\n• COD cross-dataset\n• Cross-domain constraints",
        "3. Bradley-Terry Fitting\nLogistic Regression\n• Penalty: None (C=1e8)\n• Solver: SAGA\n• Binary classification formulation",
        "4. Balanced Calibration\n• ECDF on balanced subset\n• Gamma correction (γ=0.8)\n• Isotonic regression",
        "5. Score Mapping\nFinal scores: z × 10\nRange: [0, 10]\nBoundary: 5.0"
    ]
    
    y_positions = np.linspace(0.8, 0.2, len(steps))
    
    for i, (step, y_pos) in enumerate(zip(steps, y_positions)):
        # Draw box
        rect = Rectangle((0.1, y_pos-0.08), 0.8, 0.12, 
                        facecolor='lightblue', edgecolor='black', alpha=0.7)
        ax.add_patch(rect)
        
        # Add text
        ax.text(0.5, y_pos-0.02, step, ha='center', va='center', fontsize=10, fontweight='bold')
        
        # Add arrow to next step
        if i < len(steps) - 1:
            ax.arrow(0.5, y_pos-0.08, 0, -0.04, head_width=0.02, head_length=0.01, 
                    fc='black', ec='black')
    
    pdf.savefig(fig, bbox_inches='tight')
    plt.close()

def create_theoretical_foundation(pdf):
    """Create theoretical foundation page."""
    fig, ax = plt.subplots(figsize=(8.5, 11))
    ax.axis('off')
    
    # Title
    ax.text(0.5, 0.95, 'THEORETICAL FOUNDATION', ha='center', va='top', 
            fontsize=18, fontweight='bold')
    
    theory_text = """
    1. BRADLEY-TERRY MODEL
    
    The Bradley-Terry model estimates the probability that item i defeats item j in a pairwise comparison:
    
    P(i > j) = exp(θᵢ) / (exp(θᵢ) + exp(θⱼ)) = σ(θᵢ - θⱼ)
    
    where θᵢ is the "strength" parameter for item i, and σ is the sigmoid function.
    
    2. LOGISTIC REGRESSION FORMULATION
    
    We reformulate as binary classification: For comparison (winner, loser), create feature vector:
    x = [0, ..., +1, ..., -1, ..., 0]  (winner gets +1, loser gets -1)
    
    The logistic regression estimates: P(y=1|x) = σ(θᵀx) = σ(θwinner - θloser)
    
    This is equivalent to the Bradley-Terry formulation with θ = model.coef_[0].
    
    3. COMPARISON GENERATION STRATEGY
    
    A. SOD Within-Image: All pairs (i,j) where Li > Lj within same image
    B. SOD Cross-Image: Level 5 vs Level 1,2 across different images  
    C. COD Cross-Dataset: 10% sample of all valid level pairs
    D. Cross-Domain: Sampled SOD > COD pairs (soft constraint)
    
    4. BALANCED CALIBRATION
    
    Problem: With |SOD| >> |COD|, standard ECDF/isotonic regression skews toward SOD.
    Solution: Learn calibration functions on balanced subset, apply to full dataset.
    
    Steps:
    1. Sample balanced subset: min(|COD|, |SOD|) from each domain
    2. Fit ECDF and isotonic regression on balanced subset
    3. Transform full dataset using learned functions
    
    5. THEORETICAL GUARANTEES
    
    • Maximum Likelihood: With penalty=None, recovers exact ML solution
    • Monotonicity: Isotonic regression preserves ranking order
    • Consistency: Bradley-Terry model is consistent estimator under mild conditions
    • Robustness: Balanced calibration eliminates distribution bias from class imbalance
    """
    
    ax.text(0.05, 0.85, theory_text, ha='left', va='top', fontsize=9, 
            fontfamily='monospace')
    
    pdf.savefig(fig, bbox_inches='tight')
    plt.close()

def create_data_analysis_page(pdf, df):
    """Create comprehensive data analysis page."""
    fig = plt.figure(figsize=(8.5, 11))
    
    # Title
    fig.suptitle('DATA ANALYSIS & SCORE DISTRIBUTION', fontsize=16, fontweight='bold', y=0.95)
    
    # Create subplots
    gs = fig.add_gridspec(3, 2, height_ratios=[1, 1, 1.2], hspace=0.3, wspace=0.3)
    
    # 1. Dataset composition
    ax1 = fig.add_subplot(gs[0, 0])
    domain_counts = df['domain'].value_counts()
    colors = ['lightcoral', 'lightblue']
    wedges, texts, autotexts = ax1.pie(domain_counts.values, labels=domain_counts.index, 
                                      autopct='%1.1f%%', colors=colors, startangle=90)
    ax1.set_title('Dataset Composition\n(Total: 38,580 samples)', fontweight='bold')
    
    # 2. Score distribution histogram
    ax2 = fig.add_subplot(gs[0, 1])
    cod_scores = df[df['domain'] == 'COD']['score']
    sod_scores = df[df['domain'] == 'SOD']['score']
    
    ax2.hist(cod_scores, bins=50, alpha=0.7, label='COD', color='lightcoral', density=True)
    ax2.hist(sod_scores, bins=50, alpha=0.7, label='SOD', color='lightblue', density=True)
    ax2.axvline(x=5, color='red', linestyle='--', label='Boundary')
    ax2.set_xlabel('Score')
    ax2.set_ylabel('Density')
    ax2.set_title('Score Distribution', fontweight='bold')
    ax2.legend()
    
    # 3. Box plot by score ranges
    ax3 = fig.add_subplot(gs[1, :])
    
    # Create score range categories
    df_plot = df.copy()
    df_plot['score_range'] = pd.cut(df_plot['score'], bins=[0, 2, 4, 6, 8, 10], 
                                   labels=['0-2', '2-4', '4-6', '6-8', '8-10'])
    
    sns.boxplot(data=df_plot, x='score_range', y='score', hue='domain', ax=ax3)
    ax3.set_title('Score Distribution by Range and Domain', fontweight='bold')
    ax3.set_xlabel('Score Range')
    ax3.set_ylabel('Score')
    
    # 4. Statistics table
    ax4 = fig.add_subplot(gs[2, :])
    ax4.axis('off')
    
    # Calculate statistics
    stats_data = []
    for domain in ['COD', 'SOD']:
        domain_scores = df[df['domain'] == domain]['score']
        stats_data.append([
            domain,
            f"{len(domain_scores):,}",
            f"{domain_scores.min():.4f}",
            f"{domain_scores.max():.4f}",
            f"{domain_scores.mean():.4f}",
            f"{domain_scores.std():.4f}",
            f"{domain_scores.median():.4f}"
        ])
    
    # Boundary analysis
    cod_above_5 = (cod_scores > 5).sum()
    sod_below_5 = (sod_scores < 5).sum()
    stats_data.append(['Boundary', 'Analysis', f'COD>5: {cod_above_5}', f'SOD<5: {sod_below_5}', 
                      f'{100*cod_above_5/len(cod_scores):.2f}%', f'{100*sod_below_5/len(sod_scores):.2f}%', ''])
    
    headers = ['Domain', 'Count', 'Min', 'Max', 'Mean', 'Std', 'Median']
    
    # Create table
    table = ax4.table(cellText=stats_data, colLabels=headers, loc='center', cellLoc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)
    
    # Style the table
    for i in range(len(headers)):
        table[(0, i)].set_facecolor('#40466e')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    ax4.set_title('Statistical Summary', fontweight='bold', pad=20)
    
    pdf.savefig(fig, bbox_inches='tight')
    plt.close()

def create_comparison_analysis_page(pdf):
    """Create comparison generation analysis page."""
    fig = plt.figure(figsize=(8.5, 11))
    fig.suptitle('COMPARISON GENERATION ANALYSIS', fontsize=16, fontweight='bold', y=0.95)
    
    # Comparison counts (from the actual run)
    comparison_data = {
        'SOD Within-Image': 64010,
        'SOD Cross-Image (L5 vs L1/L2)': 2000000,
        'COD Cross-Dataset (10% sample)': 1726247,
        'Cross-Domain (SOD > COD)': 2000000
    }
    
    total_comparisons = sum(comparison_data.values())
    
    # Create pie chart
    ax1 = plt.subplot(2, 2, 1)
    colors = ['lightgreen', 'lightcoral', 'lightblue', 'lightyellow']
    wedges, texts, autotexts = ax1.pie(comparison_data.values(), labels=comparison_data.keys(), 
                                      autopct='%1.1f%%', colors=colors, startangle=90)
    ax1.set_title(f'Comparison Distribution\n(Total: {total_comparisons:,})', fontweight='bold')
    
    # Bar chart with absolute numbers
    ax2 = plt.subplot(2, 2, 2)
    bars = ax2.bar(range(len(comparison_data)), list(comparison_data.values()), color=colors)
    ax2.set_xticks(range(len(comparison_data)))
    ax2.set_xticklabels([k.replace(' ', '\n') for k in comparison_data.keys()], rotation=0, ha='center')
    ax2.set_ylabel('Number of Comparisons')
    ax2.set_title('Absolute Comparison Counts', fontweight='bold')
    ax2.set_yscale('log')
    
    # Add value labels on bars
    for bar, value in zip(bars, comparison_data.values()):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{value:,}', ha='center', va='bottom', fontsize=8)
    
    # Theoretical justification
    ax3 = plt.subplot(2, 1, 2)
    ax3.axis('off')
    
    justification_text = """
    COMPARISON GENERATION RATIONALE:
    
    1. SOD Within-Image (64K pairs):
       • Preserves relative ordering within each image
       • Ensures consistency with original annotation protocol
       • Limited to 10 pairs per image (5 choose 2)
    
    2. SOD Cross-Image (2M pairs):
       • Addresses insufficient within-image comparisons
       • Level 5 (most salient) vs Level 1,2 (least salient)
       • Provides strong cross-image ranking signal
       • Sampled to prevent overwhelming other constraints
    
    3. COD Cross-Dataset (1.7M pairs):
       • 10% sample of all valid level pairs
       • Maintains computational feasibility
       • Preserves ranking relationships across difficulty levels
    
    4. Cross-Domain Soft Constraint (2M pairs):
       • Enforces SOD > COD statistical tendency
       • Sampled to balance with other comparison sets
       • Creates "soft boundary" around score 5.0
       • Allows for ambiguous cases near the boundary
    
    BALANCE ACHIEVED:
    • All comparison sets are of similar magnitude (1-2M pairs)
    • No single constraint dominates the optimization
    • Preserves both within-domain and cross-domain relationships
    """
    
    ax3.text(0.05, 0.95, justification_text, ha='left', va='top', fontsize=9, 
            fontfamily='monospace', transform=ax3.transAxes)
    
    pdf.savefig(fig, bbox_inches='tight')
    plt.close()

def create_correlation_analysis_page(pdf, df, metrics):
    """Create correlation analysis page."""
    fig = plt.figure(figsize=(8.5, 11))
    fig.suptitle('CORRELATION ANALYSIS & MODEL VALIDATION', fontsize=16, fontweight='bold', y=0.95)
    
    # Extract level from filename for analysis
    def extract_level_from_filename(row):
        """Extract the original level from filename."""
        mask_path = row['mask_path']
        filename = Path(mask_path).stem
        level_str = filename.split('_')[-1]
        
        if row['domain'] == 'COD':
            v_to_level = {255: 1, 204: 2, 153: 3, 102: 4, 51: 5}
        else:  # SOD
            v_to_level = {153: 1, 178: 2, 204: 3, 229: 4, 255: 5}
        
        try:
            v = int(level_str)
            return v_to_level.get(v, None)
        except:
            return None
    
    df_analysis = df.copy()
    df_analysis['level'] = df_analysis.apply(extract_level_from_filename, axis=1)
    df_analysis = df_analysis.dropna(subset=['level'])
    
    # 1. Correlation scatter plots
    ax1 = plt.subplot(2, 2, 1)
    cod_data = df_analysis[df_analysis['domain'] == 'COD']
    ax1.scatter(cod_data['level'], cod_data['score'], alpha=0.6, color='lightcoral')
    ax1.set_xlabel('Discrete Level')
    ax1.set_ylabel('Continuous Score')
    ax1.set_title(f'COD: Score vs Level\n(ρ = {metrics["rho_cod"]:.4f})', fontweight='bold')
    ax1.grid(True, alpha=0.3)
    
    ax2 = plt.subplot(2, 2, 2)
    sod_data = df_analysis[df_analysis['domain'] == 'SOD']
    # For SOD, we need to group by image for proper correlation calculation
    sod_data['img_base'] = sod_data['mask_path'].apply(lambda x: Path(x).stem.rsplit('_', 1)[0])
    ax2.scatter(sod_data['level'], sod_data['score'], alpha=0.6, color='lightblue')
    ax2.set_xlabel('Discrete Level')
    ax2.set_ylabel('Continuous Score')
    ax2.set_title(f'SOD: Score vs Level\n(ρ = {metrics["rho_sod"]:.4f})', fontweight='bold')
    ax2.grid(True, alpha=0.3)
    
    # 2. Box plots by level
    ax3 = plt.subplot(2, 2, 3)
    sns.boxplot(data=cod_data, x='level', y='score', ax=ax3, color='lightcoral')
    ax3.set_title('COD Score Distribution by Level', fontweight='bold')
    ax3.set_xlabel('Discrete Level')
    ax3.set_ylabel('Score')
    
    ax4 = plt.subplot(2, 2, 4)
    sns.boxplot(data=sod_data, x='level', y='score', ax=ax4, color='lightblue')
    ax4.set_title('SOD Score Distribution by Level', fontweight='bold')
    ax4.set_xlabel('Discrete Level')
    ax4.set_ylabel('Score')
    
    pdf.savefig(fig, bbox_inches='tight')
    plt.close()

def create_calibration_analysis_page(pdf, ecdf_params):
    """Create calibration process analysis page."""
    fig = plt.figure(figsize=(8.5, 11))
    fig.suptitle('CALIBRATION PROCESS ANALYSIS', fontsize=16, fontweight='bold', y=0.95)
    
    # 1. ECDF visualization
    ax1 = plt.subplot(2, 2, 1)
    ecdf_x = np.array(ecdf_params['x'])
    ecdf_y = np.array(ecdf_params['y'])
    
    ax1.plot(ecdf_x, ecdf_y, 'b-', linewidth=2, label='ECDF')
    ax1.plot(ecdf_x, ecdf_x, 'r--', alpha=0.7, label='y=x (uniform)')
    ax1.set_xlabel('Centered Weight')
    ax1.set_ylabel('Cumulative Probability')
    ax1.set_title('Empirical CDF\n(Balanced Subset)', fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Gamma correction effect
    ax2 = plt.subplot(2, 2, 2)
    q_values = np.linspace(0, 1, 1000)
    gamma = 0.8
    q_gamma = q_values ** gamma
    
    ax2.plot(q_values, q_values, 'b-', label='Original (γ=1.0)', linewidth=2)
    ax2.plot(q_values, q_gamma, 'r-', label=f'Corrected (γ={gamma})', linewidth=2)
    ax2.set_xlabel('Original Quantile')
    ax2.set_ylabel('Transformed Quantile')
    ax2.set_title('Gamma Correction Effect', fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Calibration process explanation
    ax3 = plt.subplot(2, 1, 2)
    ax3.axis('off')
    
    calibration_text = """
    BALANCED CALIBRATION METHODOLOGY:
    
    PROBLEM: Class Imbalance Bias
    • Dataset: 32,005 SOD vs 6,575 COD samples (4.9:1 ratio)
    • Standard ECDF/Isotonic: SOD dominates quantile space
    • Result: COD compressed to bottom 15-20% of score range
    
    SOLUTION: Balanced Calibration
    1. Create Balanced Subset:
       • Sample min(|COD|, |SOD|) = 6,575 from each domain
       • Ensures equal representation in calibration
    
    2. Learn Calibration Functions:
       • ECDF: Maps raw weights to quantiles [0,1]
       • Gamma Correction: q^γ with γ=0.8 (slight compression of extremes)
       • Isotonic Regression: Enforces monotonicity and uniformity
    
    3. Apply to Full Dataset:
       • Transform all 38,580 samples using learned functions
       • Preserves ranking while ensuring fair distribution
    
    THEORETICAL JUSTIFICATION:
    • Calibration functions learned on balanced data are unbiased
    • Isotonic regression preserves all ranking relationships
    • Final scores maintain perfect correlation with relative ordering
    
    EMPIRICAL VALIDATION:
    • COD scores: [0.0008, 5.0000], Mean: 2.50, Std: 1.44
    • SOD scores: [5.0000, 10.0000], Mean: 7.50, Std: 1.45
    • Perfect boundary separation at score 5.0
    • Uniform distribution within each domain
    """
    
    ax3.text(0.05, 0.95, calibration_text, ha='left', va='top', fontsize=9, 
            fontfamily='monospace', transform=ax3.transAxes)
    
    pdf.savefig(fig, bbox_inches='tight')
    plt.close()

def create_uniformity_analysis_page(pdf, df, metrics):
    """Create uniformity analysis page."""
    fig = plt.figure(figsize=(8.5, 11))
    fig.suptitle('UNIFORMITY ANALYSIS & DISTRIBUTION QUALITY', fontsize=16, fontweight='bold', y=0.95)
    
    cod_scores = df[df['domain'] == 'COD']['score']
    sod_scores = df[df['domain'] == 'SOD']['score']
    
    # 1. Histogram with uniform reference
    ax1 = plt.subplot(2, 2, 1)
    
    # COD histogram
    hist_cod, bins_cod = np.histogram(cod_scores, bins=25, range=(0, 5), density=True)
    bin_centers_cod = (bins_cod[:-1] + bins_cod[1:]) / 2
    ax1.bar(bin_centers_cod, hist_cod, width=0.18, alpha=0.7, color='lightcoral', label='COD')
    
    # Uniform reference for COD
    uniform_density_cod = 1/5  # Uniform over [0,5]
    ax1.axhline(y=uniform_density_cod, color='red', linestyle='--', alpha=0.7, label='Uniform (COD)')
    
    ax1.set_xlabel('Score')
    ax1.set_ylabel('Density')
    ax1.set_title(f'COD Distribution vs Uniform\nKL Divergence: {metrics["kl_div_cod"]:.6f}', fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. SOD histogram
    ax2 = plt.subplot(2, 2, 2)
    
    hist_sod, bins_sod = np.histogram(sod_scores, bins=25, range=(5, 10), density=True)
    bin_centers_sod = (bins_sod[:-1] + bins_sod[1:]) / 2
    ax2.bar(bin_centers_sod, hist_sod, width=0.18, alpha=0.7, color='lightblue', label='SOD')
    
    # Uniform reference for SOD
    uniform_density_sod = 1/5  # Uniform over [5,10]
    ax2.axhline(y=uniform_density_sod, color='blue', linestyle='--', alpha=0.7, label='Uniform (SOD)')
    
    ax2.set_xlabel('Score')
    ax2.set_ylabel('Density')
    ax2.set_title(f'SOD Distribution vs Uniform\nKL Divergence: {metrics["kl_div_sod"]:.6f}', fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. QQ plots against uniform
    ax3 = plt.subplot(2, 2, 3)
    
    # COD QQ plot
    cod_quantiles = np.sort(cod_scores)
    uniform_quantiles_cod = np.linspace(0, 5, len(cod_quantiles))
    ax3.scatter(uniform_quantiles_cod, cod_quantiles, alpha=0.6, color='lightcoral', s=1)
    ax3.plot([0, 5], [0, 5], 'r--', alpha=0.7, label='Perfect Uniform')
    ax3.set_xlabel('Theoretical Uniform Quantiles')
    ax3.set_ylabel('Observed COD Quantiles')
    ax3.set_title('COD Q-Q Plot vs Uniform', fontweight='bold')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. SOD QQ plot
    ax4 = plt.subplot(2, 2, 4)
    
    sod_quantiles = np.sort(sod_scores)
    uniform_quantiles_sod = np.linspace(5, 10, len(sod_quantiles))
    ax4.scatter(uniform_quantiles_sod, sod_quantiles, alpha=0.6, color='lightblue', s=1)
    ax4.plot([5, 10], [5, 10], 'b--', alpha=0.7, label='Perfect Uniform')
    ax4.set_xlabel('Theoretical Uniform Quantiles')
    ax4.set_ylabel('Observed SOD Quantiles')
    ax4.set_title('SOD Q-Q Plot vs Uniform', fontweight='bold')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    pdf.savefig(fig, bbox_inches='tight')
    plt.close()

def create_reliability_analysis_page(pdf, df, metrics):
    """Create reliability and validation analysis page."""
    fig = plt.figure(figsize=(8.5, 11))
    fig.suptitle('RELIABILITY ANALYSIS & VALIDATION', fontsize=16, fontweight='bold', y=0.95)
    
    # 1. Correlation metrics summary
    ax1 = plt.subplot(2, 2, 1)
    ax1.axis('off')
    
    metrics_text = f"""
    CORRELATION METRICS:
    
    Spearman ρ (COD): {metrics['rho_cod']:.6f}
    Spearman ρ (SOD): {metrics['rho_sod']:.6f}
    
    KL Divergence (COD): {metrics['kl_div_cod']:.6f}
    KL Divergence (SOD): {metrics['kl_div_sod']:.6f}
    
    INTERPRETATION:
    • ρ ≈ 0.98-1.00: Excellent correlation
    • KL < 0.01: Near-perfect uniformity
    • Both domains exceed quality thresholds
    """
    
    ax1.text(0.1, 0.9, metrics_text, ha='left', va='top', fontsize=12, 
            fontfamily='monospace', transform=ax1.transAxes,
            bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgreen", alpha=0.3))
    
    # 2. Boundary analysis
    ax2 = plt.subplot(2, 2, 2)
    
    # Create boundary region analysis
    boundary_scores = df[(df['score'] >= 4.5) & (df['score'] <= 5.5)]
    cod_near_boundary = len(boundary_scores[boundary_scores['domain'] == 'COD'])
    sod_near_boundary = len(boundary_scores[boundary_scores['domain'] == 'SOD'])
    
    boundary_data = [cod_near_boundary, sod_near_boundary]
    labels = ['COD', 'SOD']
    colors = ['lightcoral', 'lightblue']
    
    bars = ax2.bar(labels, boundary_data, color=colors)
    ax2.set_ylabel('Count')
    ax2.set_title('Samples Near Boundary\n(Score ∈ [4.5, 5.5])', fontweight='bold')
    
    # Add value labels
    for bar, value in zip(bars, boundary_data):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{value}', ha='center', va='bottom')
    
    # 3. Score range distribution
    ax3 = plt.subplot(2, 1, 2)
    
    # Create detailed range analysis
    ranges = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10)]
    cod_counts = []
    sod_counts = []
    
    for low, high in ranges:
        cod_in_range = ((df['score'] >= low) & (df['score'] < high) & (df['domain'] == 'COD')).sum()
        sod_in_range = ((df['score'] >= low) & (df['score'] < high) & (df['domain'] == 'SOD')).sum()
        cod_counts.append(cod_in_range)
        sod_counts.append(sod_in_range)
    
    x = np.arange(len(ranges))
    width = 0.35
    
    ax3.bar(x - width/2, cod_counts, width, label='COD', color='lightcoral', alpha=0.7)
    ax3.bar(x + width/2, sod_counts, width, label='SOD', color='lightblue', alpha=0.7)
    
    ax3.set_xlabel('Score Range')
    ax3.set_ylabel('Count')
    ax3.set_title('Distribution Across Score Ranges', fontweight='bold')
    ax3.set_xticks(x)
    ax3.set_xticklabels([f'{low}-{high}' for low, high in ranges], rotation=45)
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Add boundary line
    ax3.axvline(x=4.5, color='red', linestyle='--', alpha=0.7, label='Boundary')
    
    pdf.savefig(fig, bbox_inches='tight')
    plt.close()

def create_conclusions_page(pdf):
    """Create conclusions and future work page."""
    fig, ax = plt.subplots(figsize=(8.5, 11))
    ax.axis('off')
    
    # Title
    ax.text(0.5, 0.95, 'CONCLUSIONS & FUTURE WORK', ha='center', va='top', 
            fontsize=18, fontweight='bold')
    
    conclusions_text = """
    CONCLUSIONS:
    
    1. METHODOLOGY SUCCESS:
       ✓ Bradley-Terry model successfully unified camouflage-saliency ranking
       ✓ Balanced calibration eliminated class imbalance bias
       ✓ Achieved exceptional correlation with ground truth (ρ > 0.97)
       ✓ Generated uniform distributions within each domain
    
    2. TECHNICAL INNOVATIONS:
       ✓ Balanced comparison generation (5.79M pairs across 4 types)
       ✓ Regularization-free logistic regression for pure ML estimation
       ✓ Balanced calibration using domain-stratified sampling
       ✓ Perfect boundary separation at score 5.0
    
    3. VALIDATION RESULTS:
       ✓ COD scores: [0.0008, 5.0000], perfectly contained in [0,5]
       ✓ SOD scores: [5.0000, 10.0000], perfectly contained in [5,10]
       ✓ Near-uniform distributions (KL divergence < 0.002)
       ✓ Strong correlation preservation (ρ_COD = 0.979, ρ_SOD = 1.000)
    
    4. PRACTICAL IMPLICATIONS:
       ✓ Enables unified ranking across camouflage and saliency domains
       ✓ Provides continuous scores for fine-grained comparisons
       ✓ Maintains interpretable boundary at score 5.0
       ✓ Allows for "ambiguous" cases near the boundary (4.5-5.5)
    
    LIMITATIONS:
    
    1. Computational Complexity:
       • Bradley-Terry fitting requires O(n²) comparisons in worst case
       • Current implementation uses sampling to maintain feasibility
       • Memory requirements scale with number of items
    
    2. Domain Assumptions:
       • Assumes transitivity of preferences within domains
       • Cross-domain comparisons based on intuitive "soft constraint"
       • May not capture all nuances of human perception
    
    3. Calibration Sensitivity:
       • Balanced calibration requires sufficient samples in minority class
       • Gamma correction parameter (0.8) chosen empirically
       • Isotonic regression may over-smooth in some regions
    
    FUTURE WORK:
    
    1. ALGORITHMIC IMPROVEMENTS:
       • Investigate more efficient Bradley-Terry solvers (e.g., spectral methods)
       • Explore adaptive sampling strategies for comparison generation
       • Develop uncertainty quantification for individual scores
    
    2. VALIDATION EXTENSIONS:
       • Human evaluation studies to validate cross-domain boundary
       • Comparison with other ranking methods (TrueSkill, Elo, etc.)
       • Robustness analysis under different class imbalance ratios
    
    3. APPLICATION DOMAINS:
       • Extend to other visual perception tasks (aesthetics, complexity, etc.)
       • Multi-modal ranking incorporating other sensory modalities
       • Dynamic ranking that adapts to user preferences
    
    4. THEORETICAL ANALYSIS:
       • Formal analysis of balanced calibration properties
       • Convergence guarantees for large-scale Bradley-Terry
       • Optimal comparison generation strategies
    """
    
    ax.text(0.05, 0.85, conclusions_text, ha='left', va='top', fontsize=9, 
            fontfamily='monospace')
    
    pdf.savefig(fig, bbox_inches='tight')
    plt.close()

def generate_comprehensive_report():
    """Generate the complete PDF report."""
    
    print("Loading data...")
    df, metrics, ecdf_params, weights = load_data()
    
    print("Generating comprehensive PDF report...")
    
    with PdfPages('/HDD_16T/rsy/UEDG-master/Ranking_whole/bradley_terry_comprehensive_report.pdf') as pdf:
        print("  Creating title page...")
        create_title_page(pdf)
        
        print("  Creating methodology overview...")
        create_methodology_overview(pdf)
        
        print("  Creating theoretical foundation...")
        create_theoretical_foundation(pdf)
        
        print("  Creating data analysis...")
        create_data_analysis_page(pdf, df)
        
        print("  Creating comparison analysis...")
        create_comparison_analysis_page(pdf)
        
        print("  Creating correlation analysis...")
        create_correlation_analysis_page(pdf, df, metrics)
        
        print("  Creating calibration analysis...")
        create_calibration_analysis_page(pdf, ecdf_params)
        
        print("  Creating uniformity analysis...")
        create_uniformity_analysis_page(pdf, df, metrics)
        
        print("  Creating reliability analysis...")
        create_reliability_analysis_page(pdf, df, metrics)
        
        print("  Creating conclusions...")
        create_conclusions_page(pdf)
        
        # Set PDF metadata
        d = pdf.infodict()
        d['Title'] = 'Bradley-Terry Score Generation for Camouflage-Saliency Ranking'
        d['Author'] = 'Advanced Computer Vision Pipeline'
        d['Subject'] = 'Technical Report on Unified Ranking Methodology'
        d['Keywords'] = 'Bradley-Terry, Camouflage, Saliency, Ranking, Machine Learning'
        d['Creator'] = 'Python/Matplotlib'
    
    print("Report generated successfully!")
    print("Location: /HDD_16T/rsy/UEDG-master/Ranking_whole/bradley_terry_comprehensive_report.pdf")

if __name__ == "__main__":
    generate_comprehensive_report() 